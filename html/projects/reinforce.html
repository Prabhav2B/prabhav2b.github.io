<!DOCTYPE html>
<html lang='en'>

<head>
    <meta charset='UTF-8' />
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0' />
    <title>Human Aided Agent Training</title>
    <!-- FILL THIS -->
    <link rel="shortcut icon" href="?" />
    <link rel="icon" type="image/png" href="?" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="?">

    <link rel="stylesheet" href="../../css/style.css" />
    <link rel="stylesheet" href="../../css/project-template-style.css" />
    <link href="https://fonts.googleapis.com/css?family=Noto+Serif|Nunito&display=swap" rel="stylesheet">
    <script src="../../js/colorpicker.js"></script>
    <script src="https://kit.fontawesome.com/3753e4a20c.js" crossorigin="anonymous"></script>
</head>

<body onload="pickColor()">

    <section>
        <header>
            <nav class="topnav">
                <ul>
                    <li><a href='../../index.html'>work</a></li>
                    <li><a href='../about.html'>about</a></li>
                    <li><a href="https://www.medium.com/@prabhav_b" target="_blank">blog</a></li>
                    <li><a href='../contact.html'>contact</a></li>
                </ul>
            </nav>
        </header>
    </section>



    <div class='page'>
        <article>
            <header class='title'>

                <h1>Human Aided Agent Training</h1>
                <h4>A study on how AI Agents can become interfaces for human learning.</h4>
                <div class='img-container'>
                    <img class='banner' src='../../assets/img/banners/reinforce-banner.jpg' />
                </div>
                <div class='proj-info'>
                    <time datetime="2019-08">August 2019</time>

                    <ul class='tags'>
                        <li class='tag'><em>#ArtificialIntelligence</em></li>
                        <li class='tag'><em>#VR</em></li>
                    </ul>
                </div>
            </header>

            <hr class='section-devider' />

            <div class='main-article'>
                <div class='proj-intro'>
                    <h2>Introduction</h2>
                    <p><em>In computing, an <strong>interface</strong> is a shared boundary across which two or more separate components of a computer system exchange information. The exchange can be between software, computer hardware, peripheral devices, humans, and combinations
                        of these.</em></p>
                    <p>Through this experiment we aim at answering the question, can Artificial Intelligence serve as an interface between humans? Specifically, we asked, can an AI agent learn a task directly from an (expert) human and to what extent, and
                        if so, once trained, can it teach the task back to a (novice) human?</p>
                    <p>The project is still in progress and this serves as an account of our progress so far.</p>
                </div>
                <div class='team'>
                    <h2>Team</h2>
                    <ul>
                        <li>Prabhav Bhatnagar</li>
                        <li>Yugvir Parhar</li>
                    </ul>
                </div>
                <div class='role'>
                    <h2>My Role</h2>
                    <p>My role was primarily the design of the experiments and implementation of the same on Unity3D. I handled environment setup and programming of the functionalities along with major VR interactions.</p>
                </div>
                <div class='Tech-Stack'>
                    <h2>Tech Stack</h2>
                    <ul>

                        <li>Unity3D</li>
                        <li><a href='https://github.com/Unity-Technologies/ml-agents' target='_blank'>ml-agents</a></li>
                        <li>OcculusVR</li>
                    </ul>
                </div>

                <div class='para'>
                    <h2>Initial Research</h2>
                    <p>We started off by exploring algorithms and experiments which would be able to achieve this goal. We settled on using the <span class='pick'>ml-agents</span> package by Unity for it would allow us to easily set up our experiment and
                        explore <span class='pick'>Reinforcement Learning</span> and <span class='pick'>Imitation Learning</span>. As an abstract idea, we planned on creating a VR table tennis game that we would teach an AI agent through demonstration
                        and once trained to an appropriate level, we would test its performance against humans.</p>
                    <p>That was easier said them, after a quick prototype, we realized that the idea would be too complicated to execute, so we decided to scale down and fundamentally think about the problem. It was here that I developed my 3 phase implementation
                        technique. <span class='pick'>Toy</span>-<span class='pick'>Test</span>-<span class='pick'>Trial</span>. </p>
                </div>

                <figure>
                    <img class='para-img' src='../../assets/img/reinforce-images/reinforce-ideation.jpg' style="width: 100%; margin-left: 5%;">
                    <figcaption>Our greenboard during early stages of ideation.</figcaption>
                </figure>

                <div class='para'>
                    <h2>Breaking It Down</h2>
                    <p>Let’s talk a bit about Reinforcement Learning and Imitation Learning.</p>
                    <p>
                        <h4>Reinforcement Learning</h4>In layman terms, is the training of an agent to take actions informed by the observations from the environment and receiving an appropriate <span class='pick'>reward</span > in response. It must learn to perform actions that maximize
                        the cumulative reward, thus leading it to its desired goal.</p>
                    <p>
                        <h4>Imitation Learning</h4>On the other hand, is best summarized as <span class='pick'>‘monkey see, monkey do’</span>. It works the principle of behavioral cloning which is irrespective of rewards.</p>
                    <p>We theorized that to its core, the information the agent needed to learn was </p>
                    <ul>
                        <li>Its translational information (XYZ)</li>
                        <li>Its linear velocity</li>
                        <li>Its rotational information (yaw, pitch, roll)</li>
                        <li>It’s torque</li>

                        <li>Its position with respect to the target (ball).</li>
                    </ul>
                    <p>So let’s break this down to toy, test, and trial</p>
                    <ul>
                        <li>For the <span class='pick'>toy</span> task, we give the agent <span class='pick'>2 degrees of freedom (XZ)</span>.</li>
                        <li>For the <span class='pick'>test</span> task, we give the agent <span class='pick'>3 degrees of freedom (XYZ)</span>.</li>
                        <li>For the <span class='pick'>trial</span> task, we give the agent <span class='pick'>5 degrees of freedom (XYZ pitch and roll)</span>.</li>
                    </ul>

                    <p><em> Note: yaw is redundant owing to symmetry.</em></p>
                </div>

                <figure>
                    <img class='para-img' src='../../assets/img/reinforce-images/paddle.gif'>
                    <figcaption>The agent figuering out how to keep bouncing the ball.</figcaption>
                </figure>

                <div class='para'>
                    <h2>Experiment setup</h2>
                    <p> The Virtual Training setup contains a paddle that must learn to continuously bounce against it a pong ball without letting it drop to the floor. The paddle has a Degree of Freedom of 5, it can translate in X, Y and Z axes (3 units
                        in all the respective directions), and it has the ability to perform pitch and roll (Rotate along X, and Y axes). There are <span class='pick'>4 important colliders</span> present, on the <span class='pick'>ball</span>, on the
                        <span class='pick'>paddle</span>, on the <span class='pick'>ground</span> and an <span class='pick'>invisible collider</span> above the a certain threshold to register juggling. The ball is instantiated randomly (range of (-1 to
                        1)) above the XY plane at various angles to the plane. Rewards are provided in contact with the colliders (see "Experiment Design").
                    </p>
                    <p>The agent takes an array of five elements as controls. The elements in order are X, Y, Z axes, pitch, and roll with each being able to take a value from -1 to +1. The paddle makes constrained movements ranging from -3 to 3 units in
                        X and Z axes, 0 to 3 in the Y-axis, and the pitch and roll are not constrained. In the VR environment, for Imitation learning, the controls are mapped to the Oculus hand Controllers.</p>
                    <h4>There are a total of 15 states:</h4>
                    <ul>
                        <li>The position vector of the paddle, i.e, <span class='pick'>x[1]</span>,<span class='pick'> y[2]</span>, <span class='pick'>z[3]</span> coordinates of the paddle.</li>
                        <li>The position vector of the ball, i.e, <span class='pick'>x[4]</span>, <span class='pick'>y[5]</span>, <span class='pick'>z[6] </span>coordinates of the ball.</li>
                        <li>The velocity of the paddle, i.e, a vector3 containing <span class='pick'>magnitude[7]</span> and <span class='pick'>direction[8 9 10]</span>.</li>
                        <li>The velocity of the ball, i.e, a vector3 containing <span class='pick'>magnitude[11]</span> and <span class='pick'>direction[12 13 14]</span>.</li>
                        <li>The distance between the paddle and the <span class='pick'>ball[15]</span>.</li>
                    </ul>
                    <h4>The rules for the awarding rewards are:</h4>
                    <ul>
                        <li><span class='pick'>+5</span> rewarded when the ball hits ”PositiveRewardZone” (a collider above the ground plane).</li>
                        <li><span class='pick'>-0.1</span> for every frame the ball is being balanced on the paddle and not being jugled.</li>
                        <li><span class='pick'>-1</span> if the ball hits the ground</li>
                    </ul>
                    <p>The reward is set to 0 once the ball hits the ground and the environment is reset</p>

                    <h3>Approach/ Control Model</h3>
                    <p>The control training method employed was to use Reinforcement Learning with PPO optimization for an environment where the agent had 5 degrees of freedom. It is apparent that using simple Reinforcement Learning was too complex a problem
                        to be trained in a few hours. Thus, we broke the problem into two parts to reduce complexity. One part would be to learn the axes’ translation and another to learn the rotation aspects.</p>

                    <h3>Experimental Model 1:</h3>
                    <p>The first model consisted of using Reinforcement Learning for only part of the problem, i.e, for 3 degrees of freedom(the X, Y and Z axes translation). This model, in 300 thousand iterations performed drastically better than the control
                        which took 1 million iterations for subpar performance and a maximum score of 3.</p>

                    <h3>Experimental Model 2:</h3>
                    <p>This model was also trained for 3 dof and differed from Experimental Model 1 in that it made use of Recurrent Neural Networks in the form of <span class='pick'>"Long Short Term Memory"(LSTM)</span> architecture. It performed exceptionally
                        better than experimental model 1.</p>

                    <h3>Experimental Model 3:</h3>
                    <p>A third model is a novel approach wherein training of <span class='pick'>two different models</span> is done, one akin to ”Experimental Model 2” and the other that learns the Rotation actions (pitch and roll) via demonstrations provided
                        by the trainer, using the Virtual Reality controller to map the rotation input. Then, we use the brains simultaneously to act on the agent to perform the task.</p>
                </div>

                <div class='para'>
                    <h2>Results:</h2>

                    <p>Here is an overview of the project. I have also provided videos for the individual performances of all the models!</p>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/RHKeNNBPXxo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <p>Here is the performance of the Control Model with 5 degrees of freedom. As expected, it failed to learn the task properly due to the <span class='pick'>high complexity</span> of the problem.</p>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/JJnAn72WoL4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <p>Here is the performance of the model when given only 3 degrees of freedom (Experimental Model 2). As expected, it has learned the task completely due to the <span class='pick'>low complexity</span> of the problem.</p>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/QIsP0ibN7lk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>
                    <p>Here is the performance of the novel model (Experimental model 3) when given 5 degrees of freedom. It has learned the <span class='pick'>complex task with an improved level of performance</span> when given the same task to perform
                        in Virtual Reality.</p>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/LAQrX3Z0LDg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </figure>


                </div>

                <div class='para'>
                    <h2>Challenges:</h2>
                    <ul>
                        <li>Identifying the novel mechanism for utilizing differently trained models simultaneously that would satisfy the task proved to be very difficult, I overcame this issue by ideating through all the various split-ups of the problem.</li>
                        <li>The ML-Agents toolkit is still relatively new and does not offer as much freedom as we would have liked, for example, we could only use one type of optimizer and couldn’t experiment further. To compensate for that we used different
                            Neural Network Architectures in combination with the Optimiser to maximize the possibilities of our study.</li>
                        <li>The hyperparameter tuning took a huge portion of our timeline for the project.</li>
                        <li>Lack of online support for technical issues related to ML-Agents given its infancy.</li>
                        <li>The expert had to be comfortable while demonstrating the task. The use of mouse and keyboard proved erroneous. So, we made use of VR to enable the expert to perform the task freely and more intuitively.</li>
                    </ul>
                </div>

                <div class='div'>
                    <h2>Impact</h2>
                    <p>The results could enable the prospect of AI as an intermediary, where specialized tasks could be taught to AI agents that would in turn teach other actors. Industries which would reap great benefit from this research include but are
                        not limited to Sports, Video Games and Robotics.</p>
                </div>

                <div class='div'>
                    <h2>Achievement</h2>
                    <p>Here is my teammate <span class='pick'><a href="https://www.parharyugvir.com/" target="_blank">Yuvir Parhar</a></span> showcasing our work as a poster presentation at <span class='pick'>IndiaHCI 2019</span> and the work also won
                        <span class='pick'>Silver</span> at <span class='pick'>SRM Research Day</span>.</p>
                    <div class="img-col">
                        <figure class='img-adj'>
                            <img class='para-img' width="560" height="315" src='../../assets/img/reinforce-images/tugvir.jpeg'>
                            <figcaption>Tugvir looking very smug.</figcaption>
                        </figure>
                        <figure class='img-adj'>
                            <img class='para-img' width="560" height="315" src='../../assets/img/reinforce-images/poster.jpg'>
                            <figcaption>Our poster in it's full glory.</figcaption>
                        </figure>
                    </div>
                </div>

                <div class='div'>
                    <h2>Future Work</h2>
                    <p>With all that is done, there is quite some way to go, we plan on continuing the project by scaling the challenges and designing experiments which can allow clear testing of transfer of skills through AI. Imitation Learning on vr as
                        a constraint has been solved in the newer build of ml-agents and would allow us to fledge out the vr functionality. </p>
                </div>

            </div>


            <hr class='section-devider' />
            <footer>
                <nav class='project-navbar'>
                    <ul class='project-navbar-container'>
                        <li><a href='tactictoe.html'>&lharu; Prev</a></li>
                        <li><a href='../../index.html'>All</a></li>
                        <li><a href='matrex.html'>Next &rharu;</a></li>
                    </ul>
                </nav>
            </footer>
        </article>

        <hr/>
        <footer>
            <nav class='bottomnav'>
                <ul>
                    <li><a href='mailto:prabhav2b@gmail.com' target='_blank'><i class="fas fa-envelope"></i></a></li>
                    <li><a href='https://www.twitter.com/prabby_patty/' target='_blank'><i class="fab fa-twitter"></i></a></li>
                    <li><a href='https://www.linkedin.com/in/prabhavbhatnagar/' target='_blank'><i class="fab fa-linkedin"></i></a></li>
                    <li><a href='https://prabby-patty.itch.io/' target='_blank'><i class="fab fa-itch-io"></i></a></li>
                    <li><a href='https://github.com/prabhav2b' target='_blank'><i class="fab fa-github"></i></a></li>
                </ul>
            </nav>
            <p class='signature'><i class="fas fa-copyright"></i> Prabhav Bhatnagar 2019</p>
        </footer>
    </div>
</body>

</html>
